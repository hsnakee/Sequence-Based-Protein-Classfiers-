{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 \u2014 Ensemble Methods\n\nCombine multiple trained models via voting and stacking."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\nsys.path.insert(0, \"../src\")\nfrom ensemble import VotingEnsemble, StackingEnsemble\nfrom evaluation import ModelEvaluator\nfrom utils import load_config, set_global_seed\nimport numpy as np\nset_global_seed(42)\nconfig = load_config(\"../configs/default.yaml\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load pre-trained classical models (from notebook 03)\nimport pickle\nfrom pathlib import Path\n\nresult_dir = Path(\"../results\")\nmodels = []\nfor pkl_file in sorted(result_dir.glob(\"classical_*.pkl\")):\n    with open(pkl_file, \"rb\") as fh:\n        models.append(pickle.load(fh))\nprint(f\"Loaded {len(models)} models:\", [m.name for m in models])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Voting ensemble\n# ve = VotingEnsemble(models, strategy=\"soft\")\n# y_pred_ens = ve.predict(X_test)\n# y_proba_ens = ve.predict_proba(X_test)\n# evaluator.evaluate_model(\"VotingEnsemble_soft\", y_test, y_pred_ens, y_proba_ens)\nprint(\"Uncomment above after loading test data\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Stacking ensemble\n# se = StackingEnsemble(cv_folds=5, seed=42)\n# se.fit(models, X_train, y_train)\n# y_pred_stack = se.predict(X_test)\n# y_proba_stack = se.predict_proba(X_test)\n# evaluator.evaluate_model(\"StackingEnsemble\", y_test, y_pred_stack, y_proba_stack)\nprint(\"Uncomment above after loading train/test data\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final comparison table\n# evaluator.plot_model_comparison(\"f1_weighted\")\n# df = evaluator.comparison_table()\n# print(df)\nprint(\"Run full pipeline (notebooks 01-05) first for end-to-end ensemble comparison.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}