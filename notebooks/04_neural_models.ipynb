{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 \u2014 Neural Models\n\nTrain CNN, BiLSTM, and Transformer classifiers on one-hot encoded sequences."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\nsys.path.insert(0, \"../src\")\nfrom data_loading import build_dataset, DataSplitter\nfrom feature_extractors import build_onehot_pipeline\nfrom neural_models import build_all_neural_models\nfrom evaluation import ModelEvaluator\nfrom utils import load_config, set_global_seed\nimport numpy as np\n\nconfig = load_config(\"../configs/default.yaml\")\nset_global_seed(42)\ndataset = build_dataset(\"../data/class_a.fasta\", \"../data/class_b.fasta\", config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "splitter = DataSplitter(test_size=0.2, val_size=0.1, seed=42)\ntrain_ds, val_ds, test_ds = splitter.split(dataset)\n\noh_pipeline = build_onehot_pipeline(config)\nX_train = oh_pipeline.fit_transform(train_ds.sequences)\nX_val = oh_pipeline.transform(val_ds.sequences)\nX_test = oh_pipeline.transform(test_ds.sequences)\nseq_length = config[\"features\"][\"one_hot\"][\"max_length\"]\nprint(f\"One-hot shape: {X_train.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cw = dataset.class_weights\ncw_array = [cw[i] for i in sorted(cw)]\ntrainers = build_all_neural_models(config, seq_length=seq_length, n_classes=2, class_weights=cw_array)\nfor t in trainers:\n    print(f\"\nTraining {t.name}...\")\n    t.fit(X_train, train_ds.labels, X_val, val_ds.labels)\n    print(f\"  {t}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "evaluator = ModelEvaluator(output_dir=\"../results/plots\", label_names=dataset.label_names)\nfor t in trainers:\n    y_pred = t.predict(X_test)\n    y_proba = t.predict_proba(X_test)\n    evaluator.evaluate_model(t.name, test_ds.labels, y_pred, y_proba)\n\nprint(evaluator.comparison_table()[[\"Model\", \"accuracy\", \"f1_weighted\", \"roc_auc\"]].to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}