{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 \u2014 Classical Models\n\nTrain and evaluate classical ML classifiers."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\nsys.path.insert(0, \"../src\")\nfrom data_loading import build_dataset, DataSplitter\nfrom feature_extractors import build_feature_pipeline\nfrom classical_models import build_all_classical_models\nfrom evaluation import ModelEvaluator\nfrom utils import load_config, set_global_seed\n\nconfig = load_config(\"../configs/default.yaml\")\nset_global_seed(42)\ndataset = build_dataset(\"../data/class_a.fasta\", \"../data/class_b.fasta\", config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split and extract features\nsplitter = DataSplitter(test_size=0.2, val_size=0.1, seed=42)\ntrain_ds, val_ds, test_ds = splitter.split(dataset)\n\npipeline = build_feature_pipeline(config)\nX_train = pipeline.fit_transform(train_ds.sequences)\nX_test = pipeline.transform(test_ds.sequences)\nimport numpy as np\nX_all_train = np.vstack([pipeline.transform(train_ds.sequences), pipeline.transform(val_ds.sequences)])\ny_all_train = np.concatenate([train_ds.labels, val_ds.labels])\nprint(f\"Training features: {X_all_train.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train all classical models\nmodels = build_all_classical_models(config, seed=42, class_counts=dataset.class_counts)\nfor m in models:\n    print(f\"Training {m.name}...\")\n    m.fit(X_all_train, y_all_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate\nevaluator = ModelEvaluator(output_dir=\"../results/plots\", label_names=dataset.label_names)\nfor m in models:\n    y_pred = m.predict(X_test)\n    y_proba = m.predict_proba(X_test)\n    evaluator.evaluate_model(m.name, test_ds.labels, y_pred, y_proba)\n\ndf = evaluator.comparison_table()\nprint(df[[\"Model\", \"accuracy\", \"f1_weighted\", \"roc_auc\", \"mcc\"]].to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "evaluator.plot_model_comparison(\"f1_weighted\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}